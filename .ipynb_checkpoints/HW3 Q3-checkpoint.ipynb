{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78d02d0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant testx. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant testy. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant trainx. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant trainy. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant sizes. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BGD (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NN to recognize hand-written digits using the MNIST data\n",
    "using DelimitedFiles\n",
    "using StatsBase\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "\n",
    "# read the MNIST data\n",
    "const testx = readdlm(\"testx.csv\", ',', Int, '\\n')\n",
    "const testy = readdlm(\"testy.csv\", ',', Int, '\\n')\n",
    "const trainx = readdlm(\"trainx.csv\", ',', Int, '\\n')\n",
    "const trainy = readdlm(\"trainy.csv\", ',', Int, '\\n')\n",
    "\n",
    "const L = 3                 # number of layers including input and output\n",
    "const sizes = [784, 30, 10] # number of neurons in each layer\n",
    "\n",
    "# the activation function\n",
    "@. f(z) = 1/(1 + exp(-z))      # sigmoid activation\n",
    "@. fprime(z) = f(z) * (1-f(z))\n",
    "\n",
    "# convert a digit d to a 10-element vector\n",
    "# e.g. 6 is converted to [0,0,0,0,0,0,1,0,0,0]\n",
    "function digit2vector(d)\n",
    "    vcat( repeat([0], d), 1, repeat([0], 9-d) )\n",
    "end\n",
    "\n",
    "# a feedforward function that returns the activations\n",
    "# from each layer and the weighted inputs to each layer\n",
    "# so that they can be used during backpropagation.\n",
    "# W,b contain the weights, biases in the network.\n",
    "# x is the input of a single training example (a vector of length 784).\n",
    "function feedforward(W, b, x)\n",
    "    # TO BE COMPLETED.\n",
    "    z = [zeros(sizes[1]), zeros(sizes[2]), zeros(sizes[3])]\n",
    "    a = [zeros(sizes[1]), zeros(sizes[2]), zeros(sizes[3])]\n",
    "    a[1] = x\n",
    "    z[2] = W[1] * x + b[1]\n",
    "    a[2] = f(z[2])\n",
    "    z[3] = W[2] * a[2] + b[2]\n",
    "    a[3] = f(z[3])\n",
    "    return a, z\n",
    "end\n",
    "\n",
    "# given an input vector, return the predicted digit\n",
    "function classify(W, b, x)\n",
    "    # TO BE COMPLETED.\n",
    "    a,z = feedforward(W, b, x)\n",
    "    predicted_digit = 0\n",
    "    predicted_digit_activation = 0\n",
    "    for i=1:10\n",
    "        if a[3][i] > predicted_digit_activation\n",
    "            predicted_digit = i-1\n",
    "            predicted_digit_activation = a[3][i]\n",
    "        end\n",
    "    end\n",
    "    return predicted_digit\n",
    "end\n",
    "\n",
    "# helper function for backprop().\n",
    "# this function computes the error for a single training example.\n",
    "# W contains the weights in the network.\n",
    "# a contains the activations.\n",
    "# z contains the weighted inputs.\n",
    "# y is the correct digit.\n",
    "# returns δ = the error. the size of δ is [ 784, 30, 10 ]\n",
    "function compute_error(W, a, z, y)\n",
    "    δ = [ zeros(sizes[1]), zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "    # note that δ[1] is junk. we put it there so that the indices make sense.\n",
    "\n",
    "    # at the output layer L\n",
    "    δ[3] = -(digit2vector(y) .- a[3]) .* fprime(z[3])\n",
    "\n",
    "    # for each earlier layer L-1,L-2,..,2 (for the HW, this means only layer 2)\n",
    "    δ[2] = W[2]' * δ[3] .* fprime(z[2])\n",
    "\n",
    "    return δ\n",
    "end\n",
    "\n",
    "# helper function for backprop(). given the errors δ and the\n",
    "# activations a for a single training example, this function returns\n",
    "# the gradient components ∇W and ∇b.\n",
    "# this function implements the equations BP3 and BP4.\n",
    "function compute_gradients(δ, a)\n",
    "    # TO BE COMPLETED.\n",
    "    ∇W = [ zeros(sizes[2], sizes[1]),\n",
    "             zeros(sizes[3], sizes[2]) ]\n",
    "    ∇b = [ zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "    ∇W[1] = δ[2]*transpose(a[1])\n",
    "    ∇b[1] = δ[2]\n",
    "    ∇W[2] = δ[3]*transpose(a[2])\n",
    "    ∇b[2] = δ[3]\n",
    "    return ∇W, ∇b\n",
    "end\n",
    "\n",
    "# backpropagation. returns ∇W and ∇b for a single training example.\n",
    "function backprop(W, b, x, y)\n",
    "    (a, z) = feedforward(W, b, x)\n",
    "    δ = compute_error(W, a, z, y)\n",
    "    (∇W, ∇b) = compute_gradients(δ, a)\n",
    "    return ∇W, ∇b\n",
    "end\n",
    "\n",
    "# gradient descent algorithm.\n",
    "# W = weights in the network\n",
    "# b = biases in the network\n",
    "# batch = the indices of the observations in the batch, i.e. the rows of trainx\n",
    "# α = step size\n",
    "# λ = regularization parameter\n",
    "function GD(W, b, batch; α=0.01, λ=0.01)\n",
    "    m = length(batch)    # batch size\n",
    "\n",
    "    # data structure to accumulate the sum over the batch.\n",
    "    # in the notes and in Ng's article sumW is ΔW and sumb is Δb.\n",
    "    sumW = [ zeros(sizes[2], sizes[1]),\n",
    "             zeros(sizes[3], sizes[2]) ]\n",
    "    sumb = [ zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "    ∇W = [ zeros(sizes[2], sizes[1]),  # layer 1 to 2\n",
    "          zeros(sizes[3], sizes[2]) ] # layer 2 to 3\n",
    "    ∇b = [ zeros(sizes[2]),   # layer 2\n",
    "          zeros(sizes[3]) ]   # layer 3\n",
    "\n",
    "    # for each training example in the batch, use backprop\n",
    "    # to compute the gradients and add them to the sum\n",
    "    # THIS FUNCTION IS INCOMPLETE.\n",
    "    for i=1:m\n",
    "        x = trainx[batch[i],:]\n",
    "        y = trainy[batch[i]]\n",
    "        ∇W, ∇b = backprop(W, b, x, y)\n",
    "        sumW += ∇W\n",
    "        sumb += ∇b\n",
    "    end\n",
    "\n",
    "    # make the update to the weights and biases and take a step\n",
    "    # of gradient descent. note that we use the average gradient.\n",
    "    W = W - α*((1/m)*sumW + λ*W)\n",
    "    b = b - α*(1/m)*sumb\n",
    "\n",
    "    # return the updated weights and biases. we also return the gradients\n",
    "    return W, b, ∇W, ∇b\n",
    "end\n",
    "\n",
    "# classify the test data and compute the classification accuracy\n",
    "function accuracy(W, b) \n",
    "    ntest = length(testy)\n",
    "    yhat = zeros(Int, ntest)\n",
    "    for i in 1:ntest\n",
    "        yhat[i] = classify(W, b, testx[i,:])\n",
    "    end\n",
    "    sum(testy .== yhat)/ntest # hit rate\n",
    "end\n",
    "\n",
    "# train the neural network using batch gradient descent.\n",
    "# this is a driver function to repeatedly call GD().\n",
    "# N = number of observations in the training data.\n",
    "# m = batch size\n",
    "# α = learning rate / step size\n",
    "# λ = regularization parameter\n",
    "function BGD(N, m, epochs; α=0.01, λ=0.01) \n",
    "    # random initialization of the weights and biases\n",
    "    d = Normal(0, 1)\n",
    "    # each is a vector of 2 matrices\n",
    "    W = [ rand(d, sizes[2], sizes[1]),  # layer 1 to 2\n",
    "          rand(d, sizes[3], sizes[2]) ] # layer 2 to 3\n",
    "    b = [ rand(d, sizes[2]),   # layer 2\n",
    "          rand(d, sizes[3]) ]  # layer 3\n",
    "    ∇W = [ zeros(sizes[2], sizes[1]),  # layer 1 to 2\n",
    "          zeros(sizes[3], sizes[2]) ] # layer 2 to 3\n",
    "    ∇b = [ zeros(sizes[2]),   # layer 2\n",
    "          zeros(sizes[3]) ]   # layer 3\n",
    "\n",
    "    # THIS FUNCTION IS INCOMPLETE.\n",
    "    #\n",
    "    # you should print out messages to monitor the progress of the\n",
    "    # training. for example, you could print the epoch number and the\n",
    "    # accuracy after completion of each epoch.\n",
    "    for i in 1:epochs\n",
    "        batch_start = 1\n",
    "        batch_end = m\n",
    "        num_batches = N/m\n",
    "        batch = collect(batch_start:batch_end)\n",
    "        for j in 1:num_batches\n",
    "            W, b, ∇W, ∇b = GD(W, b, batch, α=α, λ=λ)\n",
    "            batch_start = batch_start + m\n",
    "            batch_end = batch_end + m\n",
    "            batch = collect(batch_start:batch_end)\n",
    "        end\n",
    "        println(\"epoch: \", i, \", accuracy: \", accuracy(W, b)) \n",
    "    end\n",
    "    return W, b, ∇W, ∇b\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50191c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, accuracy: 0.1434\n",
      "epoch: 2, accuracy: 0.1864\n",
      "epoch: 3, accuracy: 0.2282\n",
      "epoch: 4, accuracy: 0.2923\n",
      "epoch: 5, accuracy: 0.367\n",
      "epoch: 6, accuracy: 0.4481\n",
      "epoch: 7, accuracy: 0.5342\n",
      "epoch: 8, accuracy: 0.6093\n",
      "epoch: 9, accuracy: 0.6882\n",
      "epoch: 10, accuracy: 0.7352\n",
      "epoch: 11, accuracy: 0.7876\n",
      "epoch: 12, accuracy: 0.831\n",
      "epoch: 13, accuracy: 0.8555\n",
      "epoch: 14, accuracy: 0.8751\n",
      "epoch: 15, accuracy: 0.8873\n",
      "epoch: 16, accuracy: 0.893\n",
      "epoch: 17, accuracy: 0.8978\n",
      "epoch: 18, accuracy: 0.8966\n",
      "epoch: 19, accuracy: 0.8982\n",
      "epoch: 20, accuracy: 0.9015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[-0.002054729257847839 -0.0002632413771160021 … 0.01168977000423505 0.00850796961506093; -0.0005258110341069333 -0.005487683134411914 … 0.011861372335399923 0.0022917182150787863; … ; 0.00574914029846047 0.003223744209855617 … -0.0009772940898019895 -0.0023555221446821613; -0.011725435317082217 -0.010350120709560602 … 0.0124116899185815 0.005708423289927708], [-0.34809282643010725 -0.21694735451044755 … 0.12623793043554943 -0.18152038986360894; 0.18608913957296672 0.31601205289281215 … -0.33871735026340083 0.3536741046885771; … ; -0.32535437566742675 -0.2587736006878181 … 0.07160674657452111 -0.23545397692809056; -0.3415061761207853 -0.16060706294450464 … 0.05821835203555815 -0.2647092838700584]], [[1.2373395949501236, -1.3066808761539621, 0.3235473836038311, -0.16792332170400903, -0.06018646674486312, -1.1635500622827033, -0.08906716505267216, -0.7541810832909471, 0.2458851777458032, 1.5127465730967131  …  0.8298779381547795, -1.1845556290483528, 0.3820014142331733, -0.4021178360284628, 0.8590691384010479, -0.582341044873012, -1.5088495678677447, 0.8012879534103584, 0.8665711190090635, 0.6889429348673862], [-1.0747130687853796, -0.6747439766356904, -0.5638314088902485, -0.7324293324332757, -0.24569717574503336, -1.2424781882308473, -1.2624376925485605, -1.7084781339759445, 1.2242131838771475, -1.5196091462942007]], [[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; -0.0 -0.0 … -0.0 -0.0; 0.0 0.0 … 0.0 0.0], [2.3082443461093042e-54 2.1263857068763757e-31 … 0.0043771810116837884 7.908668163909248e-37; 2.151658704065645e-54 1.982136909427819e-31 … 0.004080243774423308 7.372163489157914e-37; … ; -5.504425514677236e-53 -5.070750745562991e-30 … -0.10438178645898014 -1.8859647550708738e-35; 2.2931854495485594e-53 2.112513249022392e-30 … 0.04348624452585655 7.857072319641587e-36]], [[1.0949990098613686e-53, 8.224011886497025e-31, 1.9207973286523088e-37, 2.5873424173537817e-13, -0.0, 9.05508400440223e-28, -1.0663897460394521e-5, 9.084916393378577e-23, 0.0, 0.00021848567991024083  …  1.6171409373787959e-21, -0.0, 4.075991674231972e-32, 9.182779583650633e-27, 0.0001388388474233178, 4.240936124928073e-17, 1.513600608530791e-27, 0.00011000652205810081, -0.0, 2.2860071310103675e-36], [0.0043771810116837884, 0.004080243774423308, 0.015606430469688005, 0.01626555501158344, 0.002112064788938154, 0.028884678279056088, 0.005421885188394334, 0.008101817160392755, -0.10438178645898014, 0.04348624452585655]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some tuning parameters\n",
    "N = length(trainy)\n",
    "m = 25       # batch size\n",
    "epochs = 20  # number of complete passes through the training data\n",
    "α = 0.01     # learning rate / step size\n",
    "λ = 0.01     # regularization parameter\n",
    "W, b, ∇W, ∇b = BGD(N, m, epochs, α=α, λ=λ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae6b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After iterating through the data 20 times in batches of 25 observations in the gradient descent algorithm, \n",
    "# I achieved an accuracy of over 90%.\n",
    "# This meanns that the neural network with the calucated weights and biases will result in the correct predicted digit\n",
    "# 90% of the time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
