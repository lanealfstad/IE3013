{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed9e6815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant testx. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant testy. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant trainx. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant trainy. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant sizes. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BGD (generic function with 1 method)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell only to source functions from Problem 3\n",
    "# NN to recognize hand-written digits using the MNIST data\n",
    "using DelimitedFiles\n",
    "using StatsBase\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "\n",
    "# read the MNIST data\n",
    "const testx = readdlm(\"testx.csv\", ',', Int, '\\n')\n",
    "const testy = readdlm(\"testy.csv\", ',', Int, '\\n')\n",
    "const trainx = readdlm(\"trainx.csv\", ',', Int, '\\n')\n",
    "const trainy = readdlm(\"trainy.csv\", ',', Int, '\\n')\n",
    "\n",
    "const L = 3                 # number of layers including input and output\n",
    "const sizes = [784, 30, 10] # number of neurons in each layer\n",
    "\n",
    "# the activation function\n",
    "@. f(z) = 1/(1 + exp(-z))      # sigmoid activation\n",
    "@. fprime(z) = f(z) * (1-f(z))\n",
    "\n",
    "# convert a digit d to a 10-element vector\n",
    "# e.g. 6 is converted to [0,0,0,0,0,0,1,0,0,0]\n",
    "function digit2vector(d)\n",
    "    vcat( repeat([0], d), 1, repeat([0], 9-d) )\n",
    "end\n",
    "\n",
    "# a feedforward function that returns the activations\n",
    "# from each layer and the weighted inputs to each layer\n",
    "# so that they can be used during backpropagation.\n",
    "# W,b contain the weights, biases in the network.\n",
    "# x is the input of a single training example (a vector of length 784).\n",
    "function feedforward(W, b, x)\n",
    "    # TO BE COMPLETED.\n",
    "    z = [zeros(sizes[1]), zeros(sizes[2]), zeros(sizes[3])]\n",
    "    a = [zeros(sizes[1]), zeros(sizes[2]), zeros(sizes[3])]\n",
    "    a[1] = x\n",
    "    z[2] = W[1] * x + b[1]\n",
    "    a[2] = f(z[2])\n",
    "    z[3] = W[2] * a[2] + b[2]\n",
    "    a[3] = f(z[3])\n",
    "    return a, z\n",
    "end\n",
    "\n",
    "# given an input vector, return the predicted digit\n",
    "function classify(W, b, x)\n",
    "    # TO BE COMPLETED.\n",
    "    a,z = feedforward(W, b, x)\n",
    "    predicted_digit = 0\n",
    "    predicted_digit_activation = 0\n",
    "    for i=1:10\n",
    "        if a[3][i] > predicted_digit_activation\n",
    "            predicted_digit = i-1\n",
    "            predicted_digit_activation = a[3][i]\n",
    "        end\n",
    "    end\n",
    "    return predicted_digit\n",
    "end\n",
    "\n",
    "# helper function for backprop().\n",
    "# this function computes the error for a single training example.\n",
    "# W contains the weights in the network.\n",
    "# a contains the activations.\n",
    "# z contains the weighted inputs.\n",
    "# y is the correct digit.\n",
    "# returns δ = the error. the size of δ is [ 784, 30, 10 ]\n",
    "function compute_error(W, a, z, y)\n",
    "    δ = [ zeros(sizes[1]), zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "    # note that δ[1] is junk. we put it there so that the indices make sense.\n",
    "\n",
    "    # at the output layer L\n",
    "    δ[3] = -(digit2vector(y) .- a[3]) .* fprime(z[3])\n",
    "\n",
    "    # for each earlier layer L-1,L-2,..,2 (for the HW, this means only layer 2)\n",
    "    δ[2] = W[2]' * δ[3] .* fprime(z[2])\n",
    "\n",
    "    return δ\n",
    "end\n",
    "\n",
    "# helper function for backprop(). given the errors δ and the\n",
    "# activations a for a single training example, this function returns\n",
    "# the gradient components ∇W and ∇b.\n",
    "# this function implements the equations BP3 and BP4.\n",
    "function compute_gradients(δ, a)\n",
    "    # TO BE COMPLETED.\n",
    "    ∇W = [ zeros(sizes[2], sizes[1]),\n",
    "             zeros(sizes[3], sizes[2]) ]\n",
    "    ∇b = [ zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "    ∇W[1] = δ[2]*transpose(a[1])\n",
    "    ∇b[1] = δ[2]\n",
    "    ∇W[2] = δ[3]*transpose(a[2])\n",
    "    ∇b[2] = δ[3]\n",
    "    return ∇W, ∇b\n",
    "end\n",
    "\n",
    "# backpropagation. returns ∇W and ∇b for a single training example.\n",
    "function backprop(W, b, x, y)\n",
    "    (a, z) = feedforward(W, b, x)\n",
    "    δ = compute_error(W, a, z, y)\n",
    "    (∇W, ∇b) = compute_gradients(δ, a)\n",
    "    return ∇W, ∇b\n",
    "end\n",
    "\n",
    "# gradient descent algorithm.\n",
    "# W = weights in the network\n",
    "# b = biases in the network\n",
    "# batch = the indices of the observations in the batch, i.e. the rows of trainx\n",
    "# α = step size\n",
    "# λ = regularization parameter\n",
    "function GD(W, b, batch; α=0.01, λ=0.01)\n",
    "    m = length(batch)    # batch size\n",
    "\n",
    "    # data structure to accumulate the sum over the batch.\n",
    "    # in the notes and in Ng's article sumW is ΔW and sumb is Δb.\n",
    "    sumW = [ zeros(sizes[2], sizes[1]),\n",
    "             zeros(sizes[3], sizes[2]) ]\n",
    "    sumb = [ zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "    ∇W = [ zeros(sizes[2], sizes[1]),  # layer 1 to 2\n",
    "          zeros(sizes[3], sizes[2]) ] # layer 2 to 3\n",
    "    ∇b = [ zeros(sizes[2]),   # layer 2\n",
    "          zeros(sizes[3]) ]   # layer 3\n",
    "\n",
    "    # for each training example in the batch, use backprop\n",
    "    # to compute the gradients and add them to the sum\n",
    "    # THIS FUNCTION IS INCOMPLETE.\n",
    "    for i=1:m\n",
    "        x = trainx[batch[i],:]\n",
    "        y = trainy[batch[i]]\n",
    "        ∇W, ∇b = backprop(W, b, x, y)\n",
    "        sumW += ∇W\n",
    "        sumb += ∇b\n",
    "    end\n",
    "\n",
    "    # make the update to the weights and biases and take a step\n",
    "    # of gradient descent. note that we use the average gradient.\n",
    "    W = W - α*((1/m)*sumW + λ*W)\n",
    "    b = b - α*(1/m)*sumb\n",
    "\n",
    "    # return the updated weights and biases. we also return the gradients\n",
    "    return W, b, ∇W, ∇b\n",
    "end\n",
    "\n",
    "# classify the test data and compute the classification accuracy\n",
    "function accuracy(W, b) \n",
    "    ntest = length(testy)\n",
    "    yhat = zeros(Int, ntest)\n",
    "    for i in 1:ntest\n",
    "        yhat[i] = classify(W, b, testx[i,:])\n",
    "    end\n",
    "    sum(testy .== yhat)/ntest # hit rate\n",
    "end\n",
    "\n",
    "# train the neural network using batch gradient descent.\n",
    "# this is a driver function to repeatedly call GD().\n",
    "# N = number of observations in the training data.\n",
    "# m = batch size\n",
    "# α = learning rate / step size\n",
    "# λ = regularization parameter\n",
    "function BGD(N, m, epochs; α=0.01, λ=0.01) \n",
    "    # random initialization of the weights and biases\n",
    "    d = Normal(0, 1)\n",
    "    # each is a vector of 2 matrices\n",
    "    W = [ rand(d, sizes[2], sizes[1]),  # layer 1 to 2\n",
    "          rand(d, sizes[3], sizes[2]) ] # layer 2 to 3\n",
    "    b = [ rand(d, sizes[2]),   # layer 2\n",
    "          rand(d, sizes[3]) ]  # layer 3\n",
    "    ∇W = [ zeros(sizes[2], sizes[1]),  # layer 1 to 2\n",
    "          zeros(sizes[3], sizes[2]) ] # layer 2 to 3\n",
    "    ∇b = [ zeros(sizes[2]),   # layer 2\n",
    "          zeros(sizes[3]) ]   # layer 3\n",
    "\n",
    "    # THIS FUNCTION IS INCOMPLETE.\n",
    "    #\n",
    "    # you should print out messages to monitor the progress of the\n",
    "    # training. for example, you could print the epoch number and the\n",
    "    # accuracy after completion of each epoch.\n",
    "    for i in 1:epochs\n",
    "        batch_start = 1\n",
    "        batch_end = m\n",
    "        num_batches = N/m\n",
    "        batch = collect(batch_start:batch_end)\n",
    "        for j in 1:num_batches\n",
    "            W, b, ∇W, ∇b = GD(W, b, batch, α=α, λ=λ)\n",
    "            batch_start = batch_start + m\n",
    "            batch_end = batch_end + m\n",
    "            batch = collect(batch_start:batch_end)\n",
    "        end\n",
    "        println(\"epoch: \", i, \", accuracy: \", accuracy(W, b)) \n",
    "    end\n",
    "    return W, b, ∇W, ∇b\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09bc21e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compare (generic function with 1 method)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Problem 4. finite-difference gradient approximation. NOTE! Be sure\n",
    "# to source the functions for problem 3 before running this code!\n",
    "\n",
    "# unroll the weights and biases into a single vector.\n",
    "# note this function will also work for unrolling the gradient.\n",
    "# note that this is hard-coded for a 3-layer NN.\n",
    "function unroll(W, b)\n",
    "    vcat(vec(W[1]), vec(W[2]), vec(b[1]), vec(b[2]))\n",
    "end\n",
    "\n",
    "# given a single vector θ, reshape the parameters into the data\n",
    "# structures that are used for backpropagation, that is, W and b, or\n",
    "# ∇w and ∇b.  note that this is hard-coded for a 3-layer NN.\n",
    "function reshape_params(θ)\n",
    "    n1 = sizes[1]  # number of nodes in layer 1\n",
    "    n2 = sizes[2]  # number of nodes in layer 2\n",
    "    n3 = sizes[3]\n",
    "    W1 = reshape(θ[1:(n2*n1)], n2, n1)\n",
    "    W2 = reshape(θ[(n2*n1 + 1):(n2*n1 + n2*n3)], n3, n2)\n",
    "    b1 = θ[(n2*n1 + n2*n3 + 1):(n2*n1 + n2*n3 + n2)]\n",
    "    b2 = θ[(n2*n1 + n2*n3 + n2 + 1):length(θ)]\n",
    "    W = [ W1, W2 ]\n",
    "    b = [ b1, b2 ]\n",
    "    return W, b\n",
    "end\n",
    "\n",
    "# evaluate the cost function for a batch of training examples\n",
    "# θ is the unrolled vector of weights and biases.\n",
    "# batch is the set of indices of the batch of training examples.\n",
    "function J(θ, batch, λ)\n",
    "    # THIS FUNCTION IS INCOMPLETE.\n",
    "\n",
    "    m = length(batch)\n",
    "    sumJ = 0.0  # to accumulate the sum for the batch.\n",
    "    # we need to pass W, b to feedforward, so we re-create W, b from θ\n",
    "    W, b = reshape_params(θ)\n",
    "    for i in 1:length(batch)\n",
    "        \n",
    "        # grab training example i\n",
    "        x = trainx[batch[i],:]\n",
    "        y = trainy[batch[i]]\n",
    "        # feedforward to obtain a, z\n",
    "        (a, z) = feedforward(W, b, x)\n",
    "        # accumulate the cost function\n",
    "        cost = 0\n",
    "        for i in 1:length(a[3])\n",
    "            cost += (a[3][i] - digit2vector(y)[i])^2\n",
    "        end\n",
    "        sumJ += 1/2 * cost\n",
    "    end\n",
    "\n",
    "    # return the cost function. note that the regularization term only\n",
    "    # applies to the weights, not the biases\n",
    "    W_vec = vcat(vec(W[1]), vec(W[2]))\n",
    "    W_sq_sum = 0\n",
    "    for i in 1:length(W_vec)\n",
    "        W_sq_sum += W_vec[i]^2\n",
    "    end\n",
    "    return (1/m)*sumJ + (λ/2)*W_sq_sum\n",
    "end\n",
    "\n",
    "# create the ith basis vector\n",
    "function e(i)\n",
    "    e = zeros(sizes[2]*sizes[1] + sizes[3]*sizes[2] + sizes[2] + sizes[3])\n",
    "    e[i] = 1\n",
    "    return e\n",
    "end\n",
    "\n",
    "θplus(v, i; ϵ=1e-4) = v .+ ϵ*e(i)\n",
    "θminus(v, i; ϵ=1e-4) = v .- ϵ*e(i)\n",
    "\n",
    "# compute the difference between the ith element of the gradient as\n",
    "# computed from backpropagation (this is ∇θ[i]) and the approximation of\n",
    "# the ith element of the gradient as obtained from finite differencing.\n",
    "# the idea is to see if the backpropagation code is correctly computing\n",
    "# the gradient of the cost function.\n",
    "function compare1(i, θ, ∇θ, batch, λ; ϵ=1e-4)\n",
    "    # i is the ith element of the unrolled gradient θ,\n",
    "    ∇θ[i] - ( J(θplus(θ, i, ϵ=ϵ), batch, λ) - J(θminus(θ, i, ϵ=ϵ), batch, λ) )/(2*ϵ)\n",
    "end\n",
    "\n",
    "# compare each element of the gradient as computed from\n",
    "# backpropagation to its estimate as obtained from finite\n",
    "# differencing.\n",
    "function compare(W, b, ∇W, ∇b, λ)\n",
    "    θ = unroll(W, b)\n",
    "    ∇θ = unroll(∇W, ∇b)\n",
    "    m = length(trainy)\n",
    "\n",
    "    # THIS FUNCTION IS INCOMPLETE.\n",
    "\n",
    "    # create a batch of 5000 training examples to evaluate the cost function.\n",
    "    # we really just need the indices of the batch.\n",
    "    batch = collect(1:5000)\n",
    "\n",
    "    # random sample of 200 gradient components to check\n",
    "    gradient_comps = rand(θ, 200)\n",
    "\n",
    "    # loop over the 200 gradient components.\n",
    "    # for each gradient component\n",
    "    #   perform finite differencing by calling compare1\n",
    "    #   if the difference exeeds 0.001\n",
    "    #      print a message\n",
    "    #   end\n",
    "    count_greater_than = 0\n",
    "    for i in 1:length(gradient_comps)\n",
    "        #println(abs(compare1(i, θ, ∇θ, batch, λ)))\n",
    "        if (abs(compare1(i, θ, ∇θ, batch, λ)) > 0.001)\n",
    "            println(\"Difference greater than tolerance\")\n",
    "            count_greater_than += 1\n",
    "        end\n",
    "    end\n",
    "    # print number of components that exceeded the tolerance of 0.001\n",
    "    println(\"Components that exceeded tolerance: \", count_greater_than)\n",
    "end\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "437c149a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, accuracy: 0.1286\n",
      "epoch: 2, accuracy: 0.1726\n",
      "epoch: 3, accuracy: 0.2175\n",
      "epoch: 4, accuracy: 0.2802\n",
      "epoch: 5, accuracy: 0.3642\n",
      "epoch: 6, accuracy: 0.4597\n",
      "epoch: 7, accuracy: 0.5438\n",
      "epoch: 8, accuracy: 0.6302\n",
      "epoch: 9, accuracy: 0.6894\n",
      "epoch: 10, accuracy: 0.7414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[0.05218685558205248 -0.04615328455930872 … 0.07436912579867265 0.01400226854917289; 0.01991570394415453 0.06711425424748656 … 0.09116544402689819 -0.0022274081562020836; … ; 0.05981490662839449 -0.02377811769703045 … 0.02395799177603432 -0.07330034056743752; 0.04760592349821825 -0.06471891003449973 … 0.029683835997908074 0.02580231297879748], [0.11709030128425567 0.023973174136958642 … 0.26268769704692485 -0.04401347560406844; 0.27845765190571703 -0.37369908870027707 … -0.23620483481932228 0.17844228687715522; … ; -0.2044620655109194 0.1328861799288891 … -0.23667606496687058 -0.22807359329839638; 0.025110438929282244 -0.03928432535245913 … -0.029777951387515565 -0.20376246751968236]], [[-0.33947801422893986, 0.11481342282295941, -0.7737906998557946, -0.9779936463312242, -0.06797301393399413, -2.4121103967615474, -0.09787261131109479, 0.605474255530258, -0.32992600277828243, 1.1131751832773682  …  0.3425962494664555, 0.1713391863345997, -0.34346031508800096, -0.4318646922943966, -1.3421103817431692, 0.21794580261912735, -0.2900665951480201, -1.3782563095883016, 1.0584915627271125, -0.4229050428292993], [-2.531140953250823, 0.3328725046426089, -1.1109813234292794, -1.7781836715821666, -1.9110233428179613, -0.8883301729672843, -1.0036230146663294, -0.32581494804598793, -2.2527008878903314, -1.2439937310816462]], [[0.0 0.0 … 0.0 0.0; -0.0 -0.0 … -0.0 -0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.034249462372501066 0.03424946237250107 … 4.7551648040518755e-76 1.0720160624452656e-99; 0.0068313209903968245 0.006831320990396826 … 9.484545125238892e-77 2.1382192075822983e-100; … ; -0.1352235746053954 -0.13522357460539544 … -1.8774320473945106e-75 -4.232529037145986e-99; 0.012472377250927267 0.01247237725092727 … 1.7316537317118665e-76 3.9038828126558325e-100]], [[4.426764464334702e-18, -0.0, -0.0, -0.0, -0.0, 9.799995765767489e-79, -0.0, -0.0, -3.0400365238000113e-55, -0.0  …  6.395425636175699e-66, -1.3669590265816219e-14, -2.932041007922895e-193, -0.0, 0.0, 3.6449318537702286e-16, -4.875647917998957e-42, -0.0002409987911816145, 4.322938180580267e-76, 1.0262548547483761e-99], [0.03424946237250107, 0.006831320990396826, 0.024747677292195333, 0.003976937809485159, 0.011006793428512816, 0.014548575383044932, 0.005759905223906346, 0.029889579542483148, -0.13522357460539544, 0.01247237725092727]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: W, b, ∇W, ∇b have been already been\n",
    "# computed. Use your code from problem 3 to do this.\n",
    "# λ should be same as the λ that was used for problem 3.\n",
    "N = length(trainy)\n",
    "m = 25       # batch size\n",
    "epochs = 10  # number of complete passes through the training data\n",
    "α = 0.01     # learning rate / step size\n",
    "λ = 0.01     # regularization parameter\n",
    "W, b, ∇W, ∇b = BGD(N, m, epochs, α=α, λ=λ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d59c8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Difference greater than tolerance\n",
      "Components that exceeded tolerance: 55\n"
     ]
    }
   ],
   "source": [
    "compare(W, b, ∇W, ∇b, λ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1095882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns that there typically around 55 values of the gradient that differ from the solution found by \n",
    "# finite-differencing by more than 0.001.\n",
    "# If the parameters of weights and basises past into the compare function had a higher amount of accuracy in their \n",
    "# prediction of the correct digit, the number of values would be lower."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
